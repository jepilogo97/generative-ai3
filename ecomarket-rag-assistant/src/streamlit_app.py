import os
import subprocess, sys
import torch
from pathlib import Path
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama
from langchain.prompts import ChatPromptTemplate
import tomllib
import streamlit as st

# ---- ConfiguraciÃ³n de entorno ----
st.set_page_config(page_title="EcoMarket Chat", page_icon="ğŸ›’")
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

# ---- Paths ----
BASE = Path(__file__).resolve().parents[1]  # /app
SETTINGS = BASE / "src" / "settings.toml"
ARTIFACTS = BASE / "artifacts" / "faiss_index"
DATA_PATH = BASE / "data" / "pedidos.json"

# ---- Cargar configuraciÃ³n ----
with open(SETTINGS, "rb") as f:
    cfg = tomllib.load(f)

role = cfg["prompts"]["role_prompt"]
instr = cfg["prompts"]["instruction_prompt"]
model_name = cfg["model"]["name"]

# ---- Bootstrap: crea el Ã­ndice si falta ----
INDEX_FAISS = ARTIFACTS / "index.faiss"
if not INDEX_FAISS.exists():
    with st.spinner("ğŸ“¦ Generando Ã­ndice FAISS (primera vez puede tardar)â€¦"):
        subprocess.run([sys.executable, str(BASE / "src" / "ingest_data.py")], check=True)

# ---- Embeddings (CPU) ----
emb = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)

# ---- Cargar FAISS ----
db = FAISS.load_local(str(ARTIFACTS), emb, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_kwargs={"k": 4})

# ---- LLM ----
llm = ChatOllama(
    model=model_name, 
    temperature=0.2,
    num_ctx=2048,      # menos memoria en runtime
    num_batch=32,
    base_url="http://host.docker.internal:11434" 
)

prompt = ChatPromptTemplate.from_template(
    "{role}\n\n{instr}\n\n>>>>>CONTENIDO<<<<<\n{context}\n<<<<<CONTENIDO>>>>>\n\nPregunta:\n{question}"
)


# ---------- UI ----------
from chat_manager import ChatManager, render_chat_sidebar, render_chat_history
render_chat_sidebar()
st.title("ğŸ›ï¸ Asistente de EcoMarket")
render_chat_history()
chat_manager = ChatManager()

# BotÃ³n opcional para reconstruir Ã­ndice desde la UI
with st.sidebar:
    if st.button("ğŸ” Reconstruir Ã­ndice FAISS"):
        with st.spinner("Reconstruyendo Ã­ndiceâ€¦"):
            subprocess.run([sys.executable, str(BASE / "src" / "ingest_data.py")], check=True)
        st.success("Ãndice reconstruido. Recarga la pÃ¡gina (Ctrl+R).")
        
# Agregar ejemplos de consultas en el sidebar
st.sidebar.markdown("### ğŸ’¡ Ejemplos de consultas:")
st.sidebar.markdown("""
**Seguimiento de pedidos:**
- Â¿DÃ³nde estÃ¡ mi pedido 20001?
- Â¿CuÃ¡ndo llega mi pedido 20003?

**PolÃ­ticas de devoluciÃ³n:**
- Â¿CuÃ¡l es el plazo para devolver?
- Â¿Puedo devolver alimentos perecederos?
- Â¿CÃ³mo solicito una devoluciÃ³n?
- Â¿QuÃ© productos no son retornables?

**Productos:**
- Â¿Puedo devolver la Laptop del pedido 20003?
- Â¿El Yogur griego acepta devoluciones?
""")

user_query = st.chat_input(placeholder="Ej: Â¿DÃ³nde estÃ¡ mi pedido 20001?")
if user_query:
    chat_manager.save_message("user", user_query)
    with st.chat_message("user"):
        st.write(user_query)

    with st.chat_message("assistant"):
        with st.spinner("ğŸ¤” Procesando tu consulta..."):
            docs = retriever.get_relevant_documents(user_query)
            context = "\n".join(d.page_content for d in docs)
            full_prompt = prompt.format(role=role, instr=instr, context=context, question=user_query)
            resp = llm.invoke(full_prompt)
            st.write(resp.content)
            chat_manager.save_message("assistant", resp.content)
